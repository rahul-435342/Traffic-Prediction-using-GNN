
import os
import subprocess
import pickle
import h5py
import numpy as np
import pandas as pd
from scipy.sparse import coo_matrix

print("=== Step 1: Installing Dependencies ===")
try:
    import torch_geometric
    print("torch-geometric is already installed.")
except ImportError:
    print("Installing torch-geometric...")
    subprocess.run(["pip", "install", "torch-geometric"], check=True)
    print("Installation complete.")

subprocess.run(["pip", "install", "h5py"], check=True)

import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_scipy_sparse_matrix
from torch_geometric.nn import GCNConv
from google.colab import drive

print("\n=== Step 2: Mounting Google Drive ===")
try:
    # ADDED: force_remount=True to ensure a fresh connection
    drive.mount('/content/drive', force_remount=True)
    print("Google Drive mounted successfully.")
except Exception as e:
    print(f"Error mounting Google Drive: {e}")
    print("Please ensure you're running this in a Colab environment and authorize the mount.")

# --- Configuration ---
DRIVE_PATH = "/content/drive/MyDrive/BTech_Project"
DATA_DIR = os.path.join(DRIVE_PATH, "data")

H5_FILE = os.path.join(DATA_DIR, "metr-la.h5")
ADJ_FILE = os.path.join(DATA_DIR, "adj_mx.pkl")

# === NEW DEBUGGING STEP ===
print("\n=== Debug Step: Listing files in your Drive ===")
print(f"Checking for directory: {DATA_DIR}")
if not os.path.exists(DATA_DIR):
    print(f"Directory NOT FOUND: {DATA_DIR}")
    print("Please check the 'BTech_Project' and 'data' folder names again for typos.")
else:
    print(f"Directory FOUND. Checking contents...")
    # This magic command (!) will list the files
    !ls -l /content/drive/MyDrive/BTech_Project/data
print("===============================================")
# === END DEBUGGING STEP ===


# Check if files exist
if not os.path.exists(H5_FILE) or not os.path.exists(ADJ_FILE):
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("! ERROR: Data files not found!")
    print(f"! Looked for: {H5_FILE}")
    print(f"! Looked for: {ADJ_FILE}")
    print("! Your screenshot looks perfect, so this is strange.")
    print("! The 'ls -l' command above should tell us what Colab is seeing.")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
else:
    print("Data files found successfully in Google Drive.")


NUM_SENSORS = 207
TIMESTEPS_IN = 12
TIMESTEPS_OUT = 12
BATCH_SIZE = 32
EPOCHS = 100
LEARNING_RATE = 0.001

# --- 1. Data Loading ---

def load_adjacency_matrix(file_path):
    """Loads the adjacency matrix from a pickle file."""
    print(f"Loading adjacency matrix from {file_path}...")
    try:
        with open(file_path, 'rb') as f:
            _, _, adj_mx = pickle.load(f, encoding='latin1')
        print("Adjacency matrix loaded successfully.")
        return adj_mx
    except FileNotFoundError:
        print(f"Error: Adjacency matrix file not found at {file_path}")
        return None
    except Exception as e:
        print(f"An error occurred loading the adjacency matrix: {e}")
        return None


def load_traffic_data(file_path):
    """Loads traffic speed data from an HDF5 file."""
    print(f"Loading traffic data from {file_path}...")
    try:
        df = pd.read_hdf(file_path)
        traffic_data = df.values
        print("Traffic data loaded successfully.")
        return traffic_data
    except FileNotFoundError:
        print(f"Error: Traffic data file not found at {file_path}")
        return None
    except Exception as e:
        print(f"An error occurred loading the traffic data: {e}")
        return None

# --- 2. Data Preprocessing ---

class TrafficDataset:
    """A custom class to handle traffic data preprocessing and normalization."""
    def __init__(self, data, mean, std):
        self.data = data
        self.mean = mean
        self.std = std

    def normalize(self):
        """Applies Z-score normalization."""
        self.data = (self.data - self.mean) / self.std

    def create_sequences(self, num_timesteps_in, num_timesteps_out):
        """Creates input sequences and corresponding targets."""
        sequences = []
        targets = []
        if self.data.ndim == 1:
            self.data = self.data.reshape(-1, 1)

        num_samples = self.data.shape[0] - num_timesteps_in - num_timesteps_out + 1
        if num_samples < 1:
            print(f"Warning: Not enough data to create sequences. Data shape: {self.data.shape}")
            return np.array([]), np.array([])

        for i in range(num_samples):
            seq = self.data[i : i + num_timesteps_in]
            target = self.data[i + num_timesteps_in : i + num_timesteps_in + num_timesteps_out]
            sequences.append(seq)
            targets.append(target)
        return np.array(sequences), np.array(targets)

def get_dataloaders(traffic_data, adj_mx, batch_size):
    """Splits data and creates PyTorch Geometric DataLoaders."""

    train_size = int(len(traffic_data) * 0.7)
    val_size = int(len(traffic_data) * 0.1)

    train_data = traffic_data[:train_size]
    val_data = traffic_data[train_size : train_size + val_size]
    test_data = traffic_data[train_size + val_size:]

    mean = np.mean(train_data)
    std = np.std(train_data)
    print(f"Data stats: Mean={mean:.4f}, Std={std:.4f}")

    train_dataset = TrafficDataset(train_data, mean, std)
    val_dataset = TrafficDataset(val_data, mean, std)
    test_dataset = TrafficDataset(test_data, mean, std)

    train_dataset.normalize()
    val_dataset.normalize()
    test_dataset.normalize()

    X_train, y_train = train_dataset.create_sequences(TIMESTEPS_IN, TIMESTEPS_OUT)
    X_val, y_val = val_dataset.create_sequences(TIMESTEPS_IN, TIMESTEPS_OUT)
    X_test, y_test = test_dataset.create_sequences(TIMESTEPS_IN, TIMESTEPS_OUT)

    print(f"Train shapes: X={X_train.shape}, y={y_train.shape}")
    print(f"Validation shapes: X={X_val.shape}, y={y_val.shape}")
    print(f"Test shapes: X={X_test.shape}, y={y_test.shape}")

    edge_index, _ = from_scipy_sparse_matrix(coo_matrix(adj_mx))

    def create_pyg_dataset(X, y):
        return [Data(x=torch.FloatTensor(X[i]).T,
                       edge_index=edge_index,
                       y=torch.FloatTensor(y[i]).T) for i in range(len(X))]

    train_pyg_dataset = create_pyg_dataset(X_train, y_train)
    val_pyg_dataset = create_pyg_dataset(X_val, y_val)
    test_pyg_dataset = create_pyg_dataset(X_test, y_test)

    train_loader = DataLoader(train_pyg_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_pyg_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_pyg_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader, mean, std

# --- 3. Model Architecture ---

class SpatioTemporalGNN(nn.Module):
    """A Spatio-Temporal GNN model with a GCN and GRU layer."""
    def __init__(self, in_channels, gru_hidden_units, out_channels):
        super(SpatioTemporalGNN, self).__init__()
        self.gcn = GCNConv(in_channels, gru_hidden_units)
        self.gru = nn.GRU(gru_hidden_units, gru_hidden_units, batch_first=True)
        self.linear = nn.Linear(gru_hidden_units, out_channels)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        batch_size = data.num_graphs
        num_nodes = data.num_nodes // batch_size

        x = x.view(batch_size, num_nodes, TIMESTEPS_IN)

        gru_inputs = []
        for t in range(TIMESTEPS_IN):
            x_t = x[:, :, t]
            x_t_flat = x_t.reshape(-1, 1)
            edge_index_batch = data.edge_index
            h = self.gcn(x_t_flat, edge_index_batch)
            h = torch.relu(h)
            h = h.view(batch_size, num_nodes, -1)
            gru_inputs.append(h.mean(dim=1))

        gru_input_seq = torch.stack(gru_inputs, dim=1)
        _, hn = self.gru(gru_input_seq)
        hn = hn.squeeze(0)
        out = self.linear(hn)
        out = out.unsqueeze(1)
        out_repeated = out.repeat(1, num_nodes, 1)
        return out_repeated.view(-1, TIMESTEPS_OUT)


# --- 4. Training & Evaluation ---

def train_epoch(model, loader, optimizer, criterion, device):
    """Training loop for one epoch."""
    model.train()
    total_loss = 0
    for data in loader:
        data = data.to(device)
        optimizer.zero_grad()
        output = model(data)
        y = data.y.view(-1, TIMESTEPS_OUT)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * data.num_graphs
    return total_loss / len(loader.dataset)

def evaluate(model, loader, criterion, device, std, mean):
    """Evaluation loop."""
    model.eval()
    total_loss_mae = 0
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            output = model(data)
            y = data.y.view(-1, TIMESTEPS_OUT)
            output_denorm = output * std + mean
            y_denorm = y * std + mean
            mae_loss = criterion(output_denorm, y_denorm)
            total_loss_mae += mae_loss.item() * data.num_graphs
    return total_loss_mae / len(loader.dataset)

# --- 5. Main Execution ---

def main():
    print("\n=== Step 3: Loading Data ===")
    adj_mx = load_adjacency_matrix(ADJ_FILE)
    traffic_data = load_traffic_data(H5_FILE)

    if adj_mx is not None and traffic_data is not None:
        print("\n=== Step 4: Preprocessing Data & Creating DataLoaders ===")
        train_loader, val_loader, test_loader, mean, std = get_dataloaders(traffic_data, adj_mx, BATCH_SIZE)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\nUsing device: {device}")

        model = SpatioTemporalGNN(in_channels=1,
                                  gru_hidden_units=64,
                                  out_channels=TIMESTEPS_OUT).to(device)

        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
        train_criterion = nn.MSELoss()
        eval_criterion = nn.L1Loss()

        print(f"\n=== Step 5: Starting Model Training ({EPOCHS} Epochs) ===")
        print(model)

        for epoch in range(1, EPOCHS + 1):
            train_loss = train_epoch(model, train_loader, optimizer, train_criterion, device)
            val_loss = evaluate(model, val_loader, eval_criterion, device, std, mean)
            print(f'Epoch {epoch:02d}, Train Loss (MSE): {train_loss:.4f}, Val MAE: {val_loss:.4f}')

        print("\nTraining finished.")
        print("\n=== Step 6: Evaluating on Test Set ===")
        test_mae = evaluate(model, test_loader, eval_criterion, device, std, mean)
        print(f'\nFinal Test MAE: {test_mae:.4f}')
        print("\n=== Project Run Complete ===")

if __name__ == '__main__':
    if 'H5_FILE' in globals() and os.path.exists(H5_FILE):
        main()
    else:
        print("\nMain execution skipped due to missing data files. Please check paths and Google Drive.")
