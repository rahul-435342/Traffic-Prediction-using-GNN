{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOpL5a/g3NSEJquYKt3zK5y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_9L82bPXsgb","executionInfo":{"status":"ok","timestamp":1761652086460,"user_tz":-330,"elapsed":1616131,"user":{"displayName":"Rahul Kumar","userId":"12757122714426873574"}},"outputId":"9cb12d95-59b3-49aa-ad52-b156055d2839"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Step 1: Installing Dependencies ===\n","torch-geometric is already installed.\n","\n","=== Step 2: Mounting Google Drive ===\n","Mounted at /content/drive\n","Google Drive mounted successfully.\n","\n","=== Debug Step: Listing files in your Drive ===\n","Checking for directory: /content/drive/MyDrive/BTech_Project/data\n","Directory FOUND. Checking contents...\n","total 56367\n","-rw------- 1 root root   680459 Oct 28 05:07 adj_mx.pkl\n","-rw------- 1 root root 57038056 Oct 28 05:13 metr-la.h5\n","===============================================\n","Data files found successfully in Google Drive.\n","\n","=== Step 3: Loading Data ===\n","Loading adjacency matrix from /content/drive/MyDrive/BTech_Project/data/adj_mx.pkl...\n","Adjacency matrix loaded successfully.\n","Loading traffic data from /content/drive/MyDrive/BTech_Project/data/metr-la.h5...\n","Traffic data loaded successfully.\n","\n","=== Step 4: Preprocessing Data & Creating DataLoaders ===\n","Data stats: Mean=54.4059, Std=19.4943\n","Train shapes: X=(23967, 12, 207), y=(23967, 12, 207)\n","Validation shapes: X=(3404, 12, 207), y=(3404, 12, 207)\n","Test shapes: X=(6832, 12, 207), y=(6832, 12, 207)\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1695610845.py:190: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  train_loader = DataLoader(train_pyg_dataset, batch_size=batch_size, shuffle=True)\n","/tmp/ipython-input-1695610845.py:191: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  val_loader = DataLoader(val_pyg_dataset, batch_size=batch_size, shuffle=False)\n","/tmp/ipython-input-1695610845.py:192: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  test_loader = DataLoader(test_pyg_dataset, batch_size=batch_size, shuffle=False)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Using device: cuda\n","\n","=== Step 5: Starting Model Training (100 Epochs) ===\n","SpatioTemporalGNN(\n","  (gcn): GCNConv(1, 64)\n","  (gru): GRU(64, 64, batch_first=True)\n","  (linear): Linear(in_features=64, out_features=12, bias=True)\n",")\n","Epoch 01, Train Loss (MSE): 0.6210, Val MAE: 10.2317\n","Epoch 02, Train Loss (MSE): 0.6050, Val MAE: 10.2271\n","Epoch 03, Train Loss (MSE): 0.6040, Val MAE: 10.4132\n","Epoch 04, Train Loss (MSE): 0.6032, Val MAE: 10.2083\n","Epoch 05, Train Loss (MSE): 0.6023, Val MAE: 9.9883\n","Epoch 06, Train Loss (MSE): 0.6020, Val MAE: 10.1726\n","Epoch 07, Train Loss (MSE): 0.6014, Val MAE: 10.0650\n","Epoch 08, Train Loss (MSE): 0.6013, Val MAE: 10.4443\n","Epoch 09, Train Loss (MSE): 0.6010, Val MAE: 10.2123\n","Epoch 10, Train Loss (MSE): 0.6005, Val MAE: 10.3024\n","Epoch 11, Train Loss (MSE): 0.6004, Val MAE: 10.0812\n","Epoch 12, Train Loss (MSE): 0.6001, Val MAE: 9.8998\n","Epoch 13, Train Loss (MSE): 0.5994, Val MAE: 10.0963\n","Epoch 14, Train Loss (MSE): 0.5995, Val MAE: 10.2261\n","Epoch 15, Train Loss (MSE): 0.5985, Val MAE: 10.2862\n","Epoch 16, Train Loss (MSE): 0.5982, Val MAE: 10.1012\n","Epoch 17, Train Loss (MSE): 0.5983, Val MAE: 10.2536\n","Epoch 18, Train Loss (MSE): 0.5979, Val MAE: 9.9347\n","Epoch 19, Train Loss (MSE): 0.5978, Val MAE: 10.0812\n","Epoch 20, Train Loss (MSE): 0.5973, Val MAE: 10.2746\n","Epoch 21, Train Loss (MSE): 0.5966, Val MAE: 10.2832\n","Epoch 22, Train Loss (MSE): 0.5964, Val MAE: 10.0920\n","Epoch 23, Train Loss (MSE): 0.5962, Val MAE: 10.2920\n","Epoch 24, Train Loss (MSE): 0.5961, Val MAE: 9.9707\n","Epoch 25, Train Loss (MSE): 0.5952, Val MAE: 10.1213\n","Epoch 26, Train Loss (MSE): 0.5952, Val MAE: 10.0656\n","Epoch 27, Train Loss (MSE): 0.5949, Val MAE: 9.9807\n","Epoch 28, Train Loss (MSE): 0.5942, Val MAE: 10.1082\n","Epoch 29, Train Loss (MSE): 0.5940, Val MAE: 10.1689\n","Epoch 30, Train Loss (MSE): 0.5933, Val MAE: 10.0255\n","Epoch 31, Train Loss (MSE): 0.5929, Val MAE: 10.0469\n","Epoch 32, Train Loss (MSE): 0.5924, Val MAE: 9.9599\n","Epoch 33, Train Loss (MSE): 0.5920, Val MAE: 9.8637\n","Epoch 34, Train Loss (MSE): 0.5927, Val MAE: 10.1694\n","Epoch 35, Train Loss (MSE): 0.5911, Val MAE: 10.1007\n","Epoch 36, Train Loss (MSE): 0.5909, Val MAE: 9.9733\n","Epoch 37, Train Loss (MSE): 0.5908, Val MAE: 9.9607\n","Epoch 38, Train Loss (MSE): 0.5907, Val MAE: 10.0864\n","Epoch 39, Train Loss (MSE): 0.5901, Val MAE: 10.3642\n","Epoch 40, Train Loss (MSE): 0.5911, Val MAE: 10.0282\n","Epoch 41, Train Loss (MSE): 0.5894, Val MAE: 10.0832\n","Epoch 42, Train Loss (MSE): 0.5887, Val MAE: 10.0611\n","Epoch 43, Train Loss (MSE): 0.5885, Val MAE: 10.1340\n","Epoch 44, Train Loss (MSE): 0.5893, Val MAE: 10.0492\n","Epoch 45, Train Loss (MSE): 0.5882, Val MAE: 10.0377\n","Epoch 46, Train Loss (MSE): 0.5887, Val MAE: 10.1374\n","Epoch 47, Train Loss (MSE): 0.5880, Val MAE: 10.0202\n","Epoch 48, Train Loss (MSE): 0.5875, Val MAE: 10.1922\n","Epoch 49, Train Loss (MSE): 0.5873, Val MAE: 10.1007\n","Epoch 50, Train Loss (MSE): 0.5867, Val MAE: 10.0771\n","Epoch 51, Train Loss (MSE): 0.5868, Val MAE: 9.8475\n","Epoch 52, Train Loss (MSE): 0.5861, Val MAE: 10.3317\n","Epoch 53, Train Loss (MSE): 0.5855, Val MAE: 10.1510\n","Epoch 54, Train Loss (MSE): 0.5864, Val MAE: 9.9752\n","Epoch 55, Train Loss (MSE): 0.5852, Val MAE: 9.9850\n","Epoch 56, Train Loss (MSE): 0.5855, Val MAE: 10.0715\n","Epoch 57, Train Loss (MSE): 0.5851, Val MAE: 10.5058\n","Epoch 58, Train Loss (MSE): 0.5840, Val MAE: 9.9957\n","Epoch 59, Train Loss (MSE): 0.5849, Val MAE: 10.2874\n","Epoch 60, Train Loss (MSE): 0.5843, Val MAE: 10.1169\n","Epoch 61, Train Loss (MSE): 0.5840, Val MAE: 10.0649\n","Epoch 62, Train Loss (MSE): 0.5832, Val MAE: 10.0223\n","Epoch 63, Train Loss (MSE): 0.5835, Val MAE: 10.0384\n","Epoch 64, Train Loss (MSE): 0.5841, Val MAE: 10.2064\n","Epoch 65, Train Loss (MSE): 0.5830, Val MAE: 10.1005\n","Epoch 66, Train Loss (MSE): 0.5837, Val MAE: 10.0809\n","Epoch 67, Train Loss (MSE): 0.5837, Val MAE: 9.9095\n","Epoch 68, Train Loss (MSE): 0.5818, Val MAE: 10.0930\n","Epoch 69, Train Loss (MSE): 0.5825, Val MAE: 9.9947\n","Epoch 70, Train Loss (MSE): 0.5830, Val MAE: 10.2871\n","Epoch 71, Train Loss (MSE): 0.5813, Val MAE: 9.9175\n","Epoch 72, Train Loss (MSE): 0.5814, Val MAE: 10.1416\n","Epoch 73, Train Loss (MSE): 0.5822, Val MAE: 10.1170\n","Epoch 74, Train Loss (MSE): 0.5811, Val MAE: 9.9553\n","Epoch 75, Train Loss (MSE): 0.5815, Val MAE: 10.0886\n","Epoch 76, Train Loss (MSE): 0.5821, Val MAE: 9.8949\n","Epoch 77, Train Loss (MSE): 0.5808, Val MAE: 10.1646\n","Epoch 78, Train Loss (MSE): 0.5801, Val MAE: 10.1626\n","Epoch 79, Train Loss (MSE): 0.5809, Val MAE: 9.7798\n","Epoch 80, Train Loss (MSE): 0.5801, Val MAE: 10.1481\n","Epoch 81, Train Loss (MSE): 0.5807, Val MAE: 10.0092\n","Epoch 82, Train Loss (MSE): 0.5793, Val MAE: 9.8994\n","Epoch 83, Train Loss (MSE): 0.5796, Val MAE: 10.1148\n","Epoch 84, Train Loss (MSE): 0.5798, Val MAE: 10.1363\n","Epoch 85, Train Loss (MSE): 0.5794, Val MAE: 10.0566\n","Epoch 86, Train Loss (MSE): 0.5790, Val MAE: 9.9854\n","Epoch 87, Train Loss (MSE): 0.5789, Val MAE: 9.9360\n","Epoch 88, Train Loss (MSE): 0.5785, Val MAE: 9.8497\n","Epoch 89, Train Loss (MSE): 0.5784, Val MAE: 10.1492\n","Epoch 90, Train Loss (MSE): 0.5781, Val MAE: 10.0015\n","Epoch 91, Train Loss (MSE): 0.5772, Val MAE: 10.1190\n","Epoch 92, Train Loss (MSE): 0.5791, Val MAE: 10.2934\n","Epoch 93, Train Loss (MSE): 0.5788, Val MAE: 9.9818\n","Epoch 94, Train Loss (MSE): 0.5769, Val MAE: 10.0230\n","Epoch 95, Train Loss (MSE): 0.5783, Val MAE: 10.1488\n","Epoch 96, Train Loss (MSE): 0.5768, Val MAE: 9.9964\n","Epoch 97, Train Loss (MSE): 0.5758, Val MAE: 10.0804\n","Epoch 98, Train Loss (MSE): 0.5775, Val MAE: 10.0165\n","Epoch 99, Train Loss (MSE): 0.5757, Val MAE: 9.9159\n","Epoch 100, Train Loss (MSE): 0.5756, Val MAE: 9.7258\n","\n","Training finished.\n","\n","=== Step 6: Evaluating on Test Set ===\n","\n","Final Test MAE: 11.1381\n","\n","=== Project Run Complete ===\n"]}],"source":["\n","import os\n","import subprocess\n","import pickle\n","import h5py\n","import numpy as np\n","import pandas as pd\n","from scipy.sparse import coo_matrix\n","\n","print(\"=== Step 1: Installing Dependencies ===\")\n","try:\n","    import torch_geometric\n","    print(\"torch-geometric is already installed.\")\n","except ImportError:\n","    print(\"Installing torch-geometric...\")\n","    subprocess.run([\"pip\", \"install\", \"torch-geometric\"], check=True)\n","    print(\"Installation complete.\")\n","\n","subprocess.run([\"pip\", \"install\", \"h5py\"], check=True)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch_geometric.data import Data, DataLoader\n","from torch_geometric.utils import from_scipy_sparse_matrix\n","from torch_geometric.nn import GCNConv\n","from google.colab import drive\n","\n","print(\"\\n=== Step 2: Mounting Google Drive ===\")\n","try:\n","    # ADDED: force_remount=True to ensure a fresh connection\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Google Drive mounted successfully.\")\n","except Exception as e:\n","    print(f\"Error mounting Google Drive: {e}\")\n","    print(\"Please ensure you're running this in a Colab environment and authorize the mount.\")\n","\n","# --- Configuration ---\n","DRIVE_PATH = \"/content/drive/MyDrive/BTech_Project\"\n","DATA_DIR = os.path.join(DRIVE_PATH, \"data\")\n","\n","H5_FILE = os.path.join(DATA_DIR, \"metr-la.h5\")\n","ADJ_FILE = os.path.join(DATA_DIR, \"adj_mx.pkl\")\n","\n","# === NEW DEBUGGING STEP ===\n","print(\"\\n=== Debug Step: Listing files in your Drive ===\")\n","print(f\"Checking for directory: {DATA_DIR}\")\n","if not os.path.exists(DATA_DIR):\n","    print(f\"Directory NOT FOUND: {DATA_DIR}\")\n","    print(\"Please check the 'BTech_Project' and 'data' folder names again for typos.\")\n","else:\n","    print(f\"Directory FOUND. Checking contents...\")\n","    # This magic command (!) will list the files\n","    !ls -l /content/drive/MyDrive/BTech_Project/data\n","print(\"===============================================\")\n","# === END DEBUGGING STEP ===\n","\n","\n","# Check if files exist\n","if not os.path.exists(H5_FILE) or not os.path.exists(ADJ_FILE):\n","    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n","    print(\"! ERROR: Data files not found!\")\n","    print(f\"! Looked for: {H5_FILE}\")\n","    print(f\"! Looked for: {ADJ_FILE}\")\n","    print(\"! Your screenshot looks perfect, so this is strange.\")\n","    print(\"! The 'ls -l' command above should tell us what Colab is seeing.\")\n","    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n","else:\n","    print(\"Data files found successfully in Google Drive.\")\n","\n","\n","NUM_SENSORS = 207\n","TIMESTEPS_IN = 12\n","TIMESTEPS_OUT = 12\n","BATCH_SIZE = 32\n","EPOCHS = 100\n","LEARNING_RATE = 0.001\n","\n","# --- 1. Data Loading ---\n","\n","def load_adjacency_matrix(file_path):\n","    \"\"\"Loads the adjacency matrix from a pickle file.\"\"\"\n","    print(f\"Loading adjacency matrix from {file_path}...\")\n","    try:\n","        with open(file_path, 'rb') as f:\n","            _, _, adj_mx = pickle.load(f, encoding='latin1')\n","        print(\"Adjacency matrix loaded successfully.\")\n","        return adj_mx\n","    except FileNotFoundError:\n","        print(f\"Error: Adjacency matrix file not found at {file_path}\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred loading the adjacency matrix: {e}\")\n","        return None\n","\n","\n","def load_traffic_data(file_path):\n","    \"\"\"Loads traffic speed data from an HDF5 file.\"\"\"\n","    print(f\"Loading traffic data from {file_path}...\")\n","    try:\n","        df = pd.read_hdf(file_path)\n","        traffic_data = df.values\n","        print(\"Traffic data loaded successfully.\")\n","        return traffic_data\n","    except FileNotFoundError:\n","        print(f\"Error: Traffic data file not found at {file_path}\")\n","        return None\n","    except Exception as e:\n","        print(f\"An error occurred loading the traffic data: {e}\")\n","        return None\n","\n","# --- 2. Data Preprocessing ---\n","\n","class TrafficDataset:\n","    \"\"\"A custom class to handle traffic data preprocessing and normalization.\"\"\"\n","    def __init__(self, data, mean, std):\n","        self.data = data\n","        self.mean = mean\n","        self.std = std\n","\n","    def normalize(self):\n","        \"\"\"Applies Z-score normalization.\"\"\"\n","        self.data = (self.data - self.mean) / self.std\n","\n","    def create_sequences(self, num_timesteps_in, num_timesteps_out):\n","        \"\"\"Creates input sequences and corresponding targets.\"\"\"\n","        sequences = []\n","        targets = []\n","        if self.data.ndim == 1:\n","            self.data = self.data.reshape(-1, 1)\n","\n","        num_samples = self.data.shape[0] - num_timesteps_in - num_timesteps_out + 1\n","        if num_samples < 1:\n","            print(f\"Warning: Not enough data to create sequences. Data shape: {self.data.shape}\")\n","            return np.array([]), np.array([])\n","\n","        for i in range(num_samples):\n","            seq = self.data[i : i + num_timesteps_in]\n","            target = self.data[i + num_timesteps_in : i + num_timesteps_in + num_timesteps_out]\n","            sequences.append(seq)\n","            targets.append(target)\n","        return np.array(sequences), np.array(targets)\n","\n","def get_dataloaders(traffic_data, adj_mx, batch_size):\n","    \"\"\"Splits data and creates PyTorch Geometric DataLoaders.\"\"\"\n","\n","    train_size = int(len(traffic_data) * 0.7)\n","    val_size = int(len(traffic_data) * 0.1)\n","\n","    train_data = traffic_data[:train_size]\n","    val_data = traffic_data[train_size : train_size + val_size]\n","    test_data = traffic_data[train_size + val_size:]\n","\n","    mean = np.mean(train_data)\n","    std = np.std(train_data)\n","    print(f\"Data stats: Mean={mean:.4f}, Std={std:.4f}\")\n","\n","    train_dataset = TrafficDataset(train_data, mean, std)\n","    val_dataset = TrafficDataset(val_data, mean, std)\n","    test_dataset = TrafficDataset(test_data, mean, std)\n","\n","    train_dataset.normalize()\n","    val_dataset.normalize()\n","    test_dataset.normalize()\n","\n","    X_train, y_train = train_dataset.create_sequences(TIMESTEPS_IN, TIMESTEPS_OUT)\n","    X_val, y_val = val_dataset.create_sequences(TIMESTEPS_IN, TIMESTEPS_OUT)\n","    X_test, y_test = test_dataset.create_sequences(TIMESTEPS_IN, TIMESTEPS_OUT)\n","\n","    print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n","    print(f\"Validation shapes: X={X_val.shape}, y={y_val.shape}\")\n","    print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}\")\n","\n","    edge_index, _ = from_scipy_sparse_matrix(coo_matrix(adj_mx))\n","\n","    def create_pyg_dataset(X, y):\n","        return [Data(x=torch.FloatTensor(X[i]).T,\n","                       edge_index=edge_index,\n","                       y=torch.FloatTensor(y[i]).T) for i in range(len(X))]\n","\n","    train_pyg_dataset = create_pyg_dataset(X_train, y_train)\n","    val_pyg_dataset = create_pyg_dataset(X_val, y_val)\n","    test_pyg_dataset = create_pyg_dataset(X_test, y_test)\n","\n","    train_loader = DataLoader(train_pyg_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_pyg_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_pyg_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader, mean, std\n","\n","# --- 3. Model Architecture ---\n","\n","class SpatioTemporalGNN(nn.Module):\n","    \"\"\"A Spatio-Temporal GNN model with a GCN and GRU layer.\"\"\"\n","    def __init__(self, in_channels, gru_hidden_units, out_channels):\n","        super(SpatioTemporalGNN, self).__init__()\n","        self.gcn = GCNConv(in_channels, gru_hidden_units)\n","        self.gru = nn.GRU(gru_hidden_units, gru_hidden_units, batch_first=True)\n","        self.linear = nn.Linear(gru_hidden_units, out_channels)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        batch_size = data.num_graphs\n","        num_nodes = data.num_nodes // batch_size\n","\n","        x = x.view(batch_size, num_nodes, TIMESTEPS_IN)\n","\n","        gru_inputs = []\n","        for t in range(TIMESTEPS_IN):\n","            x_t = x[:, :, t]\n","            x_t_flat = x_t.reshape(-1, 1)\n","            edge_index_batch = data.edge_index\n","            h = self.gcn(x_t_flat, edge_index_batch)\n","            h = torch.relu(h)\n","            h = h.view(batch_size, num_nodes, -1)\n","            gru_inputs.append(h.mean(dim=1))\n","\n","        gru_input_seq = torch.stack(gru_inputs, dim=1)\n","        _, hn = self.gru(gru_input_seq)\n","        hn = hn.squeeze(0)\n","        out = self.linear(hn)\n","        out = out.unsqueeze(1)\n","        out_repeated = out.repeat(1, num_nodes, 1)\n","        return out_repeated.view(-1, TIMESTEPS_OUT)\n","\n","\n","# --- 4. Training & Evaluation ---\n","\n","def train_epoch(model, loader, optimizer, criterion, device):\n","    \"\"\"Training loop for one epoch.\"\"\"\n","    model.train()\n","    total_loss = 0\n","    for data in loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        y = data.y.view(-1, TIMESTEPS_OUT)\n","        loss = criterion(output, y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * data.num_graphs\n","    return total_loss / len(loader.dataset)\n","\n","def evaluate(model, loader, criterion, device, std, mean):\n","    \"\"\"Evaluation loop.\"\"\"\n","    model.eval()\n","    total_loss_mae = 0\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            output = model(data)\n","            y = data.y.view(-1, TIMESTEPS_OUT)\n","            output_denorm = output * std + mean\n","            y_denorm = y * std + mean\n","            mae_loss = criterion(output_denorm, y_denorm)\n","            total_loss_mae += mae_loss.item() * data.num_graphs\n","    return total_loss_mae / len(loader.dataset)\n","\n","# --- 5. Main Execution ---\n","\n","def main():\n","    print(\"\\n=== Step 3: Loading Data ===\")\n","    adj_mx = load_adjacency_matrix(ADJ_FILE)\n","    traffic_data = load_traffic_data(H5_FILE)\n","\n","    if adj_mx is not None and traffic_data is not None:\n","        print(\"\\n=== Step 4: Preprocessing Data & Creating DataLoaders ===\")\n","        train_loader, val_loader, test_loader, mean, std = get_dataloaders(traffic_data, adj_mx, BATCH_SIZE)\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        print(f\"\\nUsing device: {device}\")\n","\n","        model = SpatioTemporalGNN(in_channels=1,\n","                                  gru_hidden_units=64,\n","                                  out_channels=TIMESTEPS_OUT).to(device)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","        train_criterion = nn.MSELoss()\n","        eval_criterion = nn.L1Loss()\n","\n","        print(f\"\\n=== Step 5: Starting Model Training ({EPOCHS} Epochs) ===\")\n","        print(model)\n","\n","        for epoch in range(1, EPOCHS + 1):\n","            train_loss = train_epoch(model, train_loader, optimizer, train_criterion, device)\n","            val_loss = evaluate(model, val_loader, eval_criterion, device, std, mean)\n","            print(f'Epoch {epoch:02d}, Train Loss (MSE): {train_loss:.4f}, Val MAE: {val_loss:.4f}')\n","\n","        print(\"\\nTraining finished.\")\n","        print(\"\\n=== Step 6: Evaluating on Test Set ===\")\n","        test_mae = evaluate(model, test_loader, eval_criterion, device, std, mean)\n","        print(f'\\nFinal Test MAE: {test_mae:.4f}')\n","        print(\"\\n=== Project Run Complete ===\")\n","\n","if __name__ == '__main__':\n","    if 'H5_FILE' in globals() and os.path.exists(H5_FILE):\n","        main()\n","    else:\n","        print(\"\\nMain execution skipped due to missing data files. Please check paths and Google Drive.\")\n"]}]}